{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To create the dataframes and files needed for our analysis, make sure that:\n",
    "\n",
    "Make sure you have the following files in `data/raw`:\n",
    "  1. [full_database.xml](https://drive.google.com/file/d/1149kYVkazq67e0vuv-_4APyqVX6yyh2p), which will represent the XML version of the DrugBank\n",
    "  2. [BindingDB_All.tsv](https://www.bindingdb.org/bind/downloads/BindingDB_All_202411_tsv.zip), which will represent the tsv version of the BindingDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from drugbank_XML_drugparser import DrugParser\n",
    "from drugbank_bindingdb_merger import DrugBank_BindingDB_Merger\n",
    "from preprocessing import Preprocessing, CleanNumericAtrributesStrategy, ColumnClean\n",
    "\n",
    "from imports import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(MERGED):\n",
    "    print(\"Merged dataset exists.\\n Loading...\")\n",
    "\n",
    "    merged_df = pd.read_pickle(MERGED)\n",
    "\n",
    "    print(\"Merged dataset loaded\")\n",
    "\n",
    "else:\n",
    "    print(\"Merged dataset doesn't exists.\\n Creating it...\")\n",
    "\n",
    "    if os.path.exists(DRUGBANK_CSV):\n",
    "        print(\"parsed_Drugbank exists...\")\n",
    "        print(\"Loading...\")\n",
    "\n",
    "        drugbank = pd.read_csv(DRUGBANK_CSV, encoding='utf-8')\n",
    "    else:\n",
    "        print(\"parsed_Drugbank doesn't exists...\")\n",
    "        print(\"Creating parsed_Drugbank.csv\")\n",
    "\n",
    "        drugparser = DrugParser(DRUGBANK_XML)\n",
    "        drugparser.parse_drugs()\n",
    "        drugbank = drugparser.save_parsed_drugs(DRUGBANK_CSV, return_df = True)\n",
    "\n",
    "        print(\"parsed_Drugbank.csv is created\")\n",
    "        print(\"DrugBank XML is parsed. \\n Loading it ...\")\n",
    "    if os.path.exists(BINDINGDB_CLEAN):\n",
    "        print(\"BindingDB clean exists...\")\n",
    "        clean_binding_df = pd.read_pickle(BINDINGDB_CLEAN)\n",
    "    else:\n",
    "        def load_BindingDB(file_path, cols):\n",
    "            return pd.read_csv(file_path, sep='\\t', header=0, usecols=cols)\n",
    "\n",
    "        \"\"\"\n",
    "        This method srips all non-numeric characters from the value.\n",
    "\n",
    "        For example, the temperature is always given as 12C or 14.2C, so we remove the C.\n",
    "\n",
    "        In come cases, mesured values are given as >10000 or <.01. In these cases, we just take the bound, as it is still better then loosing all the information by replacing it with NA.\n",
    "        \"\"\"\n",
    "        def keep_just_numeric(value):\n",
    "            if type(value) != str:\n",
    "                return pd.NA\n",
    "\n",
    "            ## All non-numeric characters (except the decimal .) should be replaced\n",
    "            cleaned_val = re.sub(r'[^\\d.]+','', str(value)) \n",
    "            if(cleaned_val == ''): # It didn't contain any number?\n",
    "                return pd.NA\n",
    "            return float(cleaned_val)\n",
    "\n",
    "        def parse_int(value):\n",
    "            try:\n",
    "                return int(value)\n",
    "            except:\n",
    "                return pd.NA\n",
    "\n",
    "        bdb_preprocessor = Preprocessing(\n",
    "            [\n",
    "                ColumnClean('Ki (nM)', 'ki', clean=keep_just_numeric),\n",
    "                ColumnClean('pH', 'ph', clean=keep_just_numeric),\n",
    "                ColumnClean('Temp (C)', 'temp', clean=keep_just_numeric),\n",
    "                ColumnClean('IC50 (nM)', 'ic50', clean=keep_just_numeric),\n",
    "                ColumnClean('Kd (nM)', 'kd', clean=keep_just_numeric),\n",
    "                ColumnClean('kon (M-1-s-1)', 'kon', clean=keep_just_numeric),\n",
    "\n",
    "                # These columns are supposed to contain strings.\n",
    "                ColumnClean('Article DOI', 'doi'),\n",
    "\n",
    "                # We use these IDs to join bindingDB and drugbank\n",
    "                ColumnClean('PubChem CID', 'pubchem_cid'),\n",
    "                ColumnClean('ChEBI ID of Ligand', 'chebi_id'),\n",
    "                ColumnClean('ChEMBL ID of Ligand', 'chembl_id'),\n",
    "                ColumnClean('DrugBank ID of Ligand', 'drugbank_id'),\n",
    "                ColumnClean('KEGG ID of Ligand', 'kegg_id'),\n",
    "                ColumnClean('ZINC ID of Ligand', 'zinc_id'),\n",
    "                ColumnClean('Ligand SMILES', 'smiles'),\n",
    "                ColumnClean('Ligand InChI Key', 'inchi_key'),\n",
    "                ColumnClean('BindingDB MonomerID', 'bindingdb_id', clean=parse_int),\n",
    "                ColumnClean('PubChem CID','pubchem_cid'),\n",
    "                ColumnClean('UniProt (SwissProt) Primary ID of Target Chain.1', 'swissprot_protein_id'),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Lodaing Binding DB...\")\n",
    "        raw_binding_df = load_BindingDB(BINDINGDB_RAW, bdb_preprocessor.get_used_old_columns())\n",
    "\n",
    "        print(\"Cleaning Binding DB...\")\n",
    "        clean_binding_df = bdb_preprocessor.transform(raw_binding_df)\n",
    "        clean_binding_df.to_pickle(BINDINGDB_CLEAN)\n",
    "\n",
    "        assert len(clean_binding_df) == len(raw_binding_df)\n",
    "        del raw_binding_df\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_binding_df['bindingdb_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_binding_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_binding_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in clean_binding_df.columns:\n",
    "    i = clean_binding_df[col].isna().mean()\n",
    "    print(f\"NA ratio in {col}: {i:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Creating merged dataset\")\n",
    "drugbank_binding_merger = DrugBank_BindingDB_Merger()\n",
    "merged_df = drugbank_binding_merger.merge(drugbank, clean_binding_df)\n",
    "\n",
    "\n",
    "merged_df.to_pickle(MERGED)\n",
    "\n",
    "print(\"Merged dataset is loaded and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_df), len(drugbank), len(clean_binding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Matched_On'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOIs\n",
    "In order to discover temporal trends, we need to fetch data about articles, from which BindingDB measurements were taken (i.e. the time when the measurement was published)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dois = clean_binding_df['doi'].dropna().unique()\n",
    "print('Number of unique dois:', len(unique_dois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_article_metadata(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()[\"message\"]\n",
    "        \n",
    "        title_list = data.get(\"title\", [])\n",
    "        title = title_list[0] if len(title_list) > 0 else pd.NA\n",
    "        abstract = data.get(\"abstract\", pd.NA)\n",
    "        # Get published year, month and day\n",
    "        published_date = data.get(\"published-print\", None)\n",
    "        if published_date:\n",
    "            published_date = published_date[\"date-parts\"][0]\n",
    "        else:\n",
    "            published_date = []\n",
    "        year = published_date[0] if len(published_date) > 0 else pd.NA\n",
    "        month = published_date[1] if len(published_date) > 1 else pd.NA\n",
    "        day = published_date[2] if len(published_date) > 2 else pd.NA\n",
    "\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"year\": year,\n",
    "            \"month\": month,\n",
    "            \"day\": day\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for DOI {doi}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "doi = np.random.choice(unique_dois)\n",
    "metadata = fetch_article_metadata(doi)\n",
    "\n",
    "if metadata:\n",
    "    print(\"Title:\", metadata[\"title\"])\n",
    "    print(\"Abstract:\", metadata[\"abstract\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DOI_DF_PATH):\n",
    "    df_dois = pd.read_pickle(DOI_DF_PATH)\n",
    "    print('Loaded doi_df from', DOI_DF_PATH)\n",
    "else:\n",
    "    print(f'No DOI dataframe found at {DOI_DF_PATH}. Creating a new one...')\n",
    "    cols = ['fetched', 'title', 'abstract', 'year', 'month', 'day']\n",
    "\n",
    "    # Create an empty dataframe with doi as index and cols with nans\n",
    "    df_dois = pd.DataFrame(index=unique_dois, columns=cols)\n",
    "    df_dois['fetched'] = False\n",
    "\n",
    "    df_dois.index.name = 'doi'\n",
    "    df_dois.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_update(doi):\n",
    "    if df_dois.loc[doi, 'fetched']:\n",
    "        return None\n",
    "\n",
    "    # Fetch article metadata with a random delay\n",
    "    article_info = fetch_article_metadata(doi)\n",
    "    if article_info is None:\n",
    "        df_dois.loc[doi, 'fetched'] = True\n",
    "        return None\n",
    "    for key, value in article_info.items():\n",
    "        df_dois.loc[doi, key] = value\n",
    "    df_dois.loc[doi, 'fetched'] = True\n",
    "    \n",
    "    # Sleep for a short, random time to avoid triggering rate limits\n",
    "    time.sleep(random.uniform(0.05, .5))\n",
    "\n",
    "if df_dois.fetched.all():\n",
    "    print('All articles were fetched. Saving df_dois')\n",
    "    df_dois.to_pickle(DOI_DF_PATH)\n",
    "else:\n",
    "    print('Not all articles were fetched. Fetching DOIS')\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            list(tqdm(executor.map(fetch_and_update, df_dois.index), total=len(df_dois.index)))\n",
    "    except: \n",
    "        print('Error, saving WIP.')\n",
    "        df_dois.to_pickle(DOI_DF_PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all the DOIs should be fetched using the API and the result saved in the dataframe\n",
    "assert df_dois['fetched'].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show percentage of missing values per column?\n",
    "# We see abstract and day (of month) are not going to be very useful, but for the rest we almost always have data\n",
    "df_dois.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, that most articles have more measuremetns in them\n",
    "df_dois['year'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
