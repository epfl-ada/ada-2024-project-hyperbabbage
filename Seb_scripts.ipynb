{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.data_paths import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "### creates graph_data.pkl file which contains the nodes and edges of the graph\n",
    "class Graphdata_Creator:\n",
    "    def __init__(self, subfolder: str):\n",
    "        self.GRAPH_DATA_PATH = 'graph_data/' + subfolder + '/'\n",
    "        self.file_name = \"graph_data\"\n",
    "        self.df = pd.read_pickle(MERGED)\n",
    "        self.cosmic_protein = pd.read_csv(COSMIC_PROTEINS, sep='\\t')\n",
    "        self.all_prots_related_to_cancer_drugs = pd.DataFrame()\n",
    "        self.direct_effect_prots = pd.DataFrame()\n",
    "    \n",
    "    ## Create the whole procudeure from preprocessing to saving the igraph object\n",
    "    def create(self):\n",
    "        self.data_preprocessing()\n",
    "        self.create_graph_data()\n",
    "        self.save_igraph()\n",
    "\n",
    "    def data_preprocessing(self):\n",
    "        bind_cancer_proteins_by_name = self.df[self.df['drugbank_protein_name'].isin(self.cosmic_protein['Gene'])]\n",
    "        bind_cancer_proteins_by_uniprot = self.df[self.df['swissprot_protein_id'].isin(self.cosmic_protein['Uniprot'])]\n",
    "\n",
    "        ## top most studies proteins\n",
    "        most_studied_proteins = ['Cytochrome P450 3A4', 'Epidermal growth factor receptor', 'Proto-oncogene tyrosine-protein kinase Src', 'Vascular endothelial growth factor receptor 2', 'Adenosine receptor A2a', 'Cytochrome P450 2C9', 'Cytochrome P450 1A2', 'Cytochrome P450 2C19', 'Cytochrome P450 2D6', 'Prostaglandin G/H synthase 1', 'Prostaglandin G/H synthase 2']\n",
    "\n",
    "        # Add most studied proteins\n",
    "        bind_cancer_proteins_by_name = pd.concat([bind_cancer_proteins_by_name, self.df[self.df['target_name'].isin(most_studied_proteins)]])\n",
    "\n",
    "        self.cosmic_protein[self.cosmic_protein['Gene'] == 'EGFR'][['Gene', 'Gene synonym', 'Uniprot']]\n",
    "\n",
    "        cancer_related_proteins = pd.concat([bind_cancer_proteins_by_name, bind_cancer_proteins_by_uniprot]).drop_duplicates()\n",
    "        cancer_related_proteins_df = self.df[self.df['target_name'].isin(cancer_related_proteins['target_name'])]\n",
    "\n",
    "        ligands_related_to_cancer_proteins = cancer_related_proteins_df.dropna(subset=['ligand_name'])\n",
    "\n",
    "        drugs_related_to_cancer_proteins = ligands_related_to_cancer_proteins.dropna(subset='drugbank_drug_class_superclass')\n",
    "        all_prots_related_to_cancer_drugs = self.df[self.df['drugbank_drug_name'].isin(drugs_related_to_cancer_proteins['drugbank_drug_name'].unique())]\n",
    "\n",
    "        direct_prots_related_target_names = cancer_related_proteins['target_name'].unique()\n",
    "\n",
    "        direct_effect_prots = all_prots_related_to_cancer_drugs[all_prots_related_to_cancer_drugs['target_name'].isin(direct_prots_related_target_names)]\n",
    "\n",
    "        self.direct_effect_prots = direct_effect_prots\n",
    "        self.all_prots_related_to_cancer_drugs = all_prots_related_to_cancer_drugs\n",
    "\n",
    "    def _get_target_name_nonmutant(self, x):\n",
    "        return x.split('[')[0]\n",
    "    \n",
    "    def create_graph_data(self):\n",
    "        self.df = pd.read_pickle(MERGED)\n",
    "\n",
    "        self.df['target_name_nonmutant'] = self.df['target_name'].apply(self._get_target_name_nonmutant)\n",
    "        self.df['target_name'].nunique(), self.df['target_name_nonmutant'].nunique()\n",
    "\n",
    "        unique_prots = self.df['target_name_nonmutant'].unique()\n",
    "\n",
    "        protein_to_doi = {prot: set() for prot in unique_prots}\n",
    "        for _, row in self.df.iterrows():\n",
    "            protein = row['target_name_nonmutant']\n",
    "            doi = row['doi']\n",
    "            protein_to_doi[protein].add(doi)\n",
    "\n",
    "\n",
    "        count = {prot: len(dois) for prot, dois in protein_to_doi.items()}\n",
    "        target_name_to_count = {row['target_name']: count[row['target_name_nonmutant']] for _, row in self.df.iterrows()}\n",
    "\n",
    "\n",
    "        # Create a list of nodes\n",
    "        nodes = []\n",
    "        node_id_map = {}  # To map node names to unique ids\n",
    "        categories = [{'name': 'Ligand'}, {'name': 'Protein'}]\n",
    "\n",
    "        # Process ligand nodes\n",
    "        ligand_nodes = self.df['ligand_name'].dropna().unique()\n",
    "        for idx, ligand in enumerate(ligand_nodes):\n",
    "            ligand = str(ligand)\n",
    "            node_id_map[ligand] = idx\n",
    "            node = {\n",
    "                'id': str(idx),\n",
    "                'Label': ligand,\n",
    "                'category': 0,  # Index of 'ligand' in categories\n",
    "                'research_count': np.median(list(target_name_to_count.values()))\n",
    "            }\n",
    "            nodes.append(node)\n",
    "\n",
    "        # Process protein nodes\n",
    "        protein_nodes = self.df['target_name'].dropna().unique()\n",
    "        for idx, protein in enumerate(protein_nodes, start=len(node_id_map)):\n",
    "            node_id_map[protein] = idx\n",
    "            node = {\n",
    "                'id': str(idx),\n",
    "                'Label': protein,\n",
    "                'category': 1,  # Index of 'protein' in categories\n",
    "                'research_count': target_name_to_count[protein]\n",
    "            }\n",
    "            nodes.append(node)\n",
    "\n",
    "        # Process edges efficiently\n",
    "        weights_ic = self.df['ic50'].values\n",
    "        weights_ec = self.df['ec50'].values\n",
    "        weights = np.where(pd.isna(weights_ic), weights_ec, weights_ic)\n",
    "\n",
    "        weights = np.log(np.log(weights))\n",
    "        max_strength = np.quantile(weights[~np.isnan(weights)], 0.95)\n",
    "        min_strength = np.quantile(weights[~np.isnan(weights)], 0.05)\n",
    "        weights = (weights - min_strength) / (max_strength - min_strength)\n",
    "        weights = np.where(np.isnan(weights), 0, weights)\n",
    "        weights = np.where(weights < 0.1, pd.NA, weights)\n",
    "\n",
    "        # Edges is df[['ligand_name', 'traget_name', weights]] where weights is not nan\n",
    "        # Don't use loop\n",
    "        edges = self.df[['ligand_name', 'target_name']].copy()\n",
    "        edges['source'] = edges['ligand_name'].map(node_id_map)\n",
    "        edges['target'] = edges['target_name'].map(node_id_map)\n",
    "        edges['sourceLabel'] = edges['ligand_name']\n",
    "        edges['targetLabel'] = edges['target_name']\n",
    "        edges['Weight'] = weights\n",
    "        edges = edges.dropna(subset=['source', 'target', 'Weight'])\n",
    "        edges['Weight'] = edges['Weight'].clip(0.1, 1)\n",
    "\n",
    "        edges = edges.to_dict(orient='records')\n",
    "\n",
    "        # Create the graph data\n",
    "        graph_data = {\n",
    "            'nodes': nodes,\n",
    "            'links': edges,\n",
    "            'categories': categories\n",
    "        }\n",
    "        \n",
    "        self.graph_data = graph_data\n",
    "    \n",
    "    def save_igraph(self):\n",
    "\n",
    "        # From the graph data, extract nodes, edges and categories\n",
    "        nodes = self.graph_data['nodes']\n",
    "        edges = self.graph_data['links']\n",
    "        categories = self.graph_data['categories']\n",
    "\n",
    "        # Create an igraph graph\n",
    "        g = ig.Graph(directed=False)  # Set directed=True if your data is directional\n",
    "        g.add_vertices(len(nodes))\n",
    "\n",
    "        # Add vertex attributes\n",
    "        for i, node in enumerate(nodes):\n",
    "            g.vs[i]['name'] = node['Label']\n",
    "            g.vs[i]['category'] = categories[node['category']]['name']\n",
    "            g.vs[i]['research_count'] = node['research_count']\n",
    "            g.vs[i]['type'] = (node['category'] == 'Ligand')\n",
    "\n",
    "        # Add edges\n",
    "        edge_list = [(int(e['source']), int(e['target'])) for e in edges]\n",
    "        g.add_edges(edge_list)\n",
    "\n",
    "        ## Add edge attributes to the edges\n",
    "        if 'Weight' in edges[0]:\n",
    "            for i, e in enumerate(edges):\n",
    "                g.es[i]['affinity'] = e['Weight']\n",
    "                g.es[i]['sourceLabel'] = e['sourceLabel']\n",
    "                g.es[i]['targetLabel'] = e['targetLabel']\n",
    "\n",
    "\n",
    "        # # g = g.subgraph_edges(g.es.select(affinity_gt=0), delete_vertices=True)\n",
    "\n",
    "        ## Recompute nodes and edges\n",
    "        nodes = []\n",
    "        edges = []\n",
    "\n",
    "        for i, v in enumerate(g.vs):\n",
    "            nodes.append({\n",
    "                'id': i,\n",
    "                'Label': v['name'],\n",
    "                'category': v['category'],\n",
    "                'type': v['type'],\n",
    "                'research_count': v['research_count']\n",
    "            })\n",
    "\n",
    "        for e in g.es:\n",
    "            edges.append({\n",
    "                'source': e.source,\n",
    "                'target': e.target,\n",
    "                'Weight': e['affinity'],\n",
    "                'sourceLabel': e['sourceLabel'],\n",
    "                'targetLabel': e['targetLabel']\n",
    "            })\n",
    "        \n",
    "        \n",
    "        all_nodes_df = pd.DataFrame(nodes)\n",
    "        all_edges_df = pd.DataFrame(edges)\n",
    "\n",
    "        ### Save igraph and edges/nodes to pickle to load into gephi\n",
    "        if not os.path.exists(self.GRAPH_DATA_PATH):\n",
    "            os.makedirs(self.GRAPH_DATA_PATH)\n",
    "        all_nodes_df.to_csv(self.GRAPH_DATA_PATH + 'all_nodes.csv', index=False)\n",
    "        all_edges_df.to_csv(self.GRAPH_DATA_PATH + 'all_edges.csv', index=False)\n",
    "        with open(self.GRAPH_DATA_PATH +'igraph_object.pkl', 'wb') as f:\n",
    "            pickle.dump(g, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m graph_data_creator \u001b[38;5;241m=\u001b[39m \u001b[43mGraphdata_Creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcreate()\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36mGraphdata_Creator.__init__\u001b[1;34m(self, subfolder)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGRAPH_DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_data/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m subfolder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMERGED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcosmic_protein \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(COSMIC_PROTEINS, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_prots_related_to_cancer_drugs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[1;32mc:\\Users\\Charaf Kamel\\.conda\\envs\\ada\\Lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhandles\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 1) try standard library Pickle\u001b[39;49;00m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;49;00m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Charaf Kamel\\.conda\\envs\\ada\\Lib\\site-packages\\pandas\\io\\common.py:157\u001b[0m, in \u001b[0;36mIOHandles.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    153\u001b[0m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m     traceback: TracebackType \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Charaf Kamel\\.conda\\envs\\ada\\Lib\\site-packages\\pandas\\io\\common.py:144\u001b[0m, in \u001b[0;36mIOHandles.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles:\n\u001b[1;32m--> 144\u001b[0m     handle\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreated_handles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_data_creator = Graphdata_Creator(\"all\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecharts.charts import Graph\n",
    "from pyecharts import options as opts\n",
    "class Graph_Visualizer():\n",
    "    def __init__(self, subfolder:str):\n",
    "        self.GRAPH_DATA_PATH = 'graph_data/' + subfolder + '/'\n",
    "        self.file_name = \"graph_data\"\n",
    "        self.igraph_object = None\n",
    "    \n",
    "    def get_igraph_object(self):\n",
    "        with open(self.GRAPH_DATA_PATH + 'igraph_object.pkl', 'rb') as f:\n",
    "            self.igraph_object = pickle.load(f)\n",
    "        if self.igraph_object is None:\n",
    "            raise Exception(\"No igraph object found\")\n",
    "    \n",
    "    def render_html(self, project_on: str):\n",
    "        proj_nodes, proj_edges = self._bipartite_projection_with_affinity(self.igraph_object, project_on=project_on)\n",
    "        graph = self._generate_echarts_graph(proj_nodes, proj_edges)\n",
    "        graph.set_global_opts(\n",
    "            title_opts=opts.TitleOpts(title=\"Cancer Drugs Projected Interaction Network\")\n",
    "        )\n",
    "        graph.render(self.GRAPH_DATA_PATH +f\"cancer_{project_on}_graph.html\")\n",
    "\n",
    "    def visualize(self):\n",
    "        self.get_igraph_object()\n",
    "        self.render_html(\"ligand\")\n",
    "        self.render_html(\"protein\")\n",
    "    \n",
    "    def _bipartite_projection_with_affinity(self, g, project_on=\"ligand\"):\n",
    "        \"\"\"\n",
    "        Compute a bipartite projection of the given graph `g` onto either the ligand or protein side.\n",
    "        Then, aggregate 'affinity' values for each projected edge.\n",
    "\n",
    "        Parameters:\n",
    "        g : igraph.Graph\n",
    "            The original bipartite graph with\n",
    "            - `g.es['affinity']` set on the bipartite edges\n",
    "        project_on : str, optional, default: 'ligand'\n",
    "            Which set of nodes to project on. \n",
    "            'ligand' projects onto ligand nodes (type=True),\n",
    "            'protein' projects onto protein nodes (type=False).\n",
    "\n",
    "        Returns:\n",
    "        proj_nodes : list of dict\n",
    "            A list of node dictionaries with 'id' and 'name'.\n",
    "        proj_edges : list of dict\n",
    "            A list of edge dictionaries with 'source', 'target', and aggregated 'affinity'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Perform bipartite projection\n",
    "        # g_ligand_proj: projection onto ligand nodes\n",
    "        # g_protein_proj: projection onto protein nodes\n",
    "        \n",
    "        g_ligand_proj, g_protein_proj = g.bipartite_projection(types=types, multiplicity=True)\n",
    "\n",
    "        # Choose which projection to work with\n",
    "        if project_on == \"ligand\":\n",
    "            g_proj = g_ligand_proj\n",
    "        elif project_on == \"protein\":\n",
    "            g_proj = g_protein_proj\n",
    "        else:\n",
    "            raise ValueError(\"project_on must be either 'ligand' or 'protein'\")\n",
    "\n",
    "        # Create lookup from node name to original index for convenience\n",
    "        name_to_index = {v['name']: v.index for v in g.vs}\n",
    "\n",
    "        # Compute aggregated affinity on projected edges\n",
    "        for e in g_proj.es:\n",
    "            source_name = g_proj.vs[e.source]['name']\n",
    "            target_name = g_proj.vs[e.target]['name']\n",
    "\n",
    "            # Map back to original graph indices\n",
    "            source_index = name_to_index[source_name]\n",
    "            target_index = name_to_index[target_name]\n",
    "\n",
    "            # Find common neighbors in original graph\n",
    "            source_neighbors = set(g.neighbors(source_index))\n",
    "            target_neighbors = set(g.neighbors(target_index))\n",
    "            shared_intermediates = source_neighbors & target_neighbors\n",
    "\n",
    "            # We do the average of the affinities\n",
    "            affinities = []\n",
    "            for p in shared_intermediates:\n",
    "                eid_source_p = g.get_eid(source_index, p)\n",
    "                eid_target_p = g.get_eid(target_index, p)\n",
    "\n",
    "                aff_source_p = g.es[eid_source_p]['affinity']\n",
    "                aff_target_p = g.es[eid_target_p]['affinity']\n",
    "\n",
    "                avg_aff = (aff_source_p + aff_target_p) / 2.0\n",
    "                affinities.append(avg_aff)\n",
    "\n",
    "            e['affinity'] = sum(affinities)/len(affinities) if affinities else None\n",
    "\n",
    "        # Retrieve nodes with their names\n",
    "        proj_nodes = []\n",
    "        for v in g_proj.vs:\n",
    "            proj_nodes.append({\n",
    "                'id': v.index,\n",
    "                'name': v['name']\n",
    "            })\n",
    "\n",
    "        # Retrieve edges with their aggregated affinity\n",
    "        proj_edges = []\n",
    "        for e in g_proj.es:\n",
    "            proj_edges.append({\n",
    "                'source': e.source,\n",
    "                'target': e.target,\n",
    "                'affinity': e['affinity']\n",
    "            })\n",
    "\n",
    "        return proj_nodes, proj_edges\n",
    "\n",
    "    \n",
    "\n",
    "    def _generate_echarts_graph(self, nodes, edges):\n",
    "        \"\"\"\n",
    "        Generate an ECharts graph from the given nodes and edges.\n",
    "\n",
    "        Parameters:\n",
    "        nodes : list of dict\n",
    "            A list of node dictionaries with 'id', 'name', and 'symbolSize'.\n",
    "        edges : list of dict\n",
    "            A list of edge dictionaries with 'source', 'target', and 'lineStyle'.\n",
    "\n",
    "        Returns:\n",
    "        graph : pyecharts.charts.Graph\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute node degrees\n",
    "        node_degrees = {node[\"id\"]: 0 for node in nodes}\n",
    "        for edge in edges:\n",
    "            node_degrees[edge[\"source\"]] += 1\n",
    "            node_degrees[edge[\"target\"]] += 1\n",
    "\n",
    "        # Assign symbol_size to each node based on its degree\n",
    "        for node in nodes:\n",
    "            degree = node_degrees[node[\"id\"]]\n",
    "            # For example, make the size proportional to degree * 5, fallback to 10 if degree = 0\n",
    "            node[\"symbolSize\"] = np.clip(degree, 0.1, 10) if degree > 0 else 0.1\n",
    "\n",
    "        # Assign edge width based on 'affinity' (if available)\n",
    "        for edge in edges:\n",
    "            weight = edge.get(\"affinity\", 0.5) * 3\n",
    "            edge[\"lineStyle\"] = {\"width\": weight}  # scale it up if needed\n",
    "\n",
    "        # Create the graph\n",
    "        graph = Graph(init_opts=opts.InitOpts(width=\"100%\", height=\"700px\"))\n",
    "        graph.add(\n",
    "            series_name=\"\",\n",
    "            nodes=nodes,\n",
    "            links=edges,\n",
    "            layout=\"force\",\n",
    "            edge_length=[100, 250],\n",
    "            repulsion=200,\n",
    "            linestyle_opts=opts.LineStyleOpts(opacity=0.5)  # Base line style\n",
    "\n",
    "        )\n",
    "\n",
    "        graph.set_series_opts(\n",
    "            label_opts=opts.LabelOpts(\n",
    "                is_show=False,  # Hide labels by default\n",
    "                position=\"right\",\n",
    "                formatter=\"{b}\"  # Use node name as the label\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mGraph_Visualizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 25\u001b[0m, in \u001b[0;36mGraph_Visualizer.visualize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_igraph_object()\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_html\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mligand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_html(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotein\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 16\u001b[0m, in \u001b[0;36mGraph_Visualizer.render_html\u001b[1;34m(self, project_on)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, project_on: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     proj_nodes, proj_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bipartite_projection_with_affinity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43migraph_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_on\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_echarts_graph(proj_nodes, proj_edges)\n\u001b[0;32m     18\u001b[0m     graph\u001b[38;5;241m.\u001b[39mset_global_opts(\n\u001b[0;32m     19\u001b[0m         title_opts\u001b[38;5;241m=\u001b[39mopts\u001b[38;5;241m.\u001b[39mTitleOpts(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCancer Drugs Projected Interaction Network\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[17], line 53\u001b[0m, in \u001b[0;36mGraph_Visualizer._bipartite_projection_with_affinity\u001b[1;34m(self, g, project_on)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03mCompute a bipartite projection of the given graph `g` onto either the ligand or protein side.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03mThen, aggregate 'affinity' values for each projected edge.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    A list of edge dictionaries with 'source', 'target', and aggregated 'affinity'.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Perform bipartite projection\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# g_ligand_proj: projection onto ligand nodes\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# g_protein_proj: projection onto protein nodes\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m g_ligand_proj, g_protein_proj \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mbipartite_projection(types\u001b[38;5;241m=\u001b[39m\u001b[43mtypes\u001b[49m, multiplicity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Choose which projection to work with\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_on \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mligand\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'types' is not defined"
     ]
    }
   ],
   "source": [
    "Graph_Visualizer(\"all\").visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
